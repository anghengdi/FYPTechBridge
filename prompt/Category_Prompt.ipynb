{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5b5e2d5773e642149a76237b432e39d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d8fb43b7a7f34365a178ea383d40563a",
              "IPY_MODEL_8111d8b30b6e42bf92bb2c196d8e34f6",
              "IPY_MODEL_377d026d39ac453aac5a3ea8c213ba38"
            ],
            "layout": "IPY_MODEL_983274f1ff034d76abff6fe9cc5d72be"
          }
        },
        "d8fb43b7a7f34365a178ea383d40563a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9884f283e60e4c6f8d747d4173170f2f",
            "placeholder": "​",
            "style": "IPY_MODEL_a5798dba1abd44b4818ecbe7493ce7f3",
            "value": "Downloading (…)2-13b-chat.Q6_K.gguf: 100%"
          }
        },
        "8111d8b30b6e42bf92bb2c196d8e34f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f7196faf17f45f491e309aaa067456a",
            "max": 10679140224,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f3cfa4d39cb841f5b554f0ea93286e4b",
            "value": 10679140224
          }
        },
        "377d026d39ac453aac5a3ea8c213ba38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a231692ff8734ef1a28ae1af74f158b6",
            "placeholder": "​",
            "style": "IPY_MODEL_599b4ea3e1114ee19bdbe8f632b04398",
            "value": " 10.7G/10.7G [01:25&lt;00:00, 195MB/s]"
          }
        },
        "983274f1ff034d76abff6fe9cc5d72be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9884f283e60e4c6f8d747d4173170f2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5798dba1abd44b4818ecbe7493ce7f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f7196faf17f45f491e309aaa067456a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3cfa4d39cb841f5b554f0ea93286e4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a231692ff8734ef1a28ae1af74f158b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "599b4ea3e1114ee19bdbe8f632b04398": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries and configuration\n",
        "\n"
      ],
      "metadata": {
        "id": "Apy3bKn9usWf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mv9uziNSjxan",
        "outputId": "75d841cd-d737-4aac-d060-b0e5df7c7422"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.6.tar.gz (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Running command pip subprocess to install build dependencies\n",
            "  Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "  Collecting scikit-build-core[pyproject]>=0.5.0\n",
            "    Downloading scikit_build_core-0.5.0-py3-none-any.whl (129 kB)\n",
            "       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129.5/129.5 kB 2.9 MB/s eta 0:00:00\n",
            "  Collecting exceptiongroup (from scikit-build-core[pyproject]>=0.5.0)\n",
            "    Downloading exceptiongroup-1.1.3-py3-none-any.whl (14 kB)\n",
            "  Collecting packaging>=20.9 (from scikit-build-core[pyproject]>=0.5.0)\n",
            "    Downloading packaging-23.1-py3-none-any.whl (48 kB)\n",
            "       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.9/48.9 kB 4.3 MB/s eta 0:00:00\n",
            "  Collecting tomli>=1.1 (from scikit-build-core[pyproject]>=0.5.0)\n",
            "    Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\n",
            "  Collecting pathspec>=0.10.1 (from scikit-build-core[pyproject]>=0.5.0)\n",
            "    Downloading pathspec-0.11.2-py3-none-any.whl (29 kB)\n",
            "  Collecting pyproject-metadata>=0.5 (from scikit-build-core[pyproject]>=0.5.0)\n",
            "    Downloading pyproject_metadata-0.7.1-py3-none-any.whl (7.4 kB)\n",
            "  Installing collected packages: tomli, pathspec, packaging, exceptiongroup, scikit-build-core, pyproject-metadata\n",
            "  Successfully installed exceptiongroup-1.1.3 packaging-23.1 pathspec-0.11.2 pyproject-metadata-0.7.1 scikit-build-core-0.5.0 tomli-2.0.1\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Running command Getting requirements to build wheel\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Running command pip subprocess to install backend dependencies\n",
            "  Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "  Collecting ninja>=1.5\n",
            "    Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 146.0/146.0 kB 3.0 MB/s eta 0:00:00\n",
            "  Collecting cmake>=3.12\n",
            "    Downloading cmake-3.27.4.1-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.1 MB)\n",
            "       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.1/26.1 MB 44.5 MB/s eta 0:00:00\n",
            "  Installing collected packages: ninja, cmake\n",
            "    Creating /tmp/pip-build-env-juvkujf5/normal/local/bin\n",
            "    changing mode of /tmp/pip-build-env-juvkujf5/normal/local/bin/ninja to 755\n",
            "    changing mode of /tmp/pip-build-env-juvkujf5/normal/local/bin/cmake to 755\n",
            "    changing mode of /tmp/pip-build-env-juvkujf5/normal/local/bin/cpack to 755\n",
            "    changing mode of /tmp/pip-build-env-juvkujf5/normal/local/bin/ctest to 755\n",
            "  Successfully installed cmake-3.27.4.1 ninja-1.11.1\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Running command Preparing metadata (pyproject.toml)\n",
            "  *** scikit-build-core 0.5.0 using CMake 3.27.4 (metadata_wheel)\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typing-extensions>=4.5.0 (from llama-cpp-python)\n",
            "  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python)\n",
            "  Downloading numpy-1.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m257.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m239.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Running command Building wheel for llama-cpp-python (pyproject.toml)\n",
            "  *** scikit-build-core 0.5.0 using CMake 3.27.4 (wheel)\n",
            "  *** Configuring CMake...\n",
            "  loading initial cache file /tmp/tmphc8as334/build/CMakeInit.txt\n",
            "  -- The C compiler identification is GNU 11.4.0\n",
            "  -- The CXX compiler identification is GNU 11.4.0\n",
            "  -- Detecting C compiler ABI info\n",
            "  -- Detecting C compiler ABI info - done\n",
            "  -- Check for working C compiler: /usr/bin/cc - skipped\n",
            "  -- Detecting C compile features\n",
            "  -- Detecting C compile features - done\n",
            "  -- Detecting CXX compiler ABI info\n",
            "  -- Detecting CXX compiler ABI info - done\n",
            "  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "  -- Detecting CXX compile features\n",
            "  -- Detecting CXX compile features - done\n",
            "  -- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "  fatal: not a git repository (or any of the parent directories): .git\n",
            "  fatal: not a git repository (or any of the parent directories): .git\n",
            "  CMake Warning at vendor/llama.cpp/CMakeLists.txt:125 (message):\n",
            "    Git repository not found; to enable automatic generation of build info,\n",
            "    make sure Git is installed and the project is a Git repository.\n",
            "\n",
            "\n",
            "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "  -- Found Threads: TRUE\n",
            "  -- Found CUDAToolkit: /usr/local/cuda/include (found version \"11.8.89\")\n",
            "  -- cuBLAS found\n",
            "  -- The CUDA compiler identification is NVIDIA 11.8.89\n",
            "  -- Detecting CUDA compiler ABI info\n",
            "  -- Detecting CUDA compiler ABI info - done\n",
            "  -- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "  -- Detecting CUDA compile features\n",
            "  -- Detecting CUDA compile features - done\n",
            "  -- Using CUDA architectures: 52;61;70\n",
            "  -- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "  -- x86 detected\n",
            "  CMake Warning (dev) at CMakeLists.txt:19 (install):\n",
            "    Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
            "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\n",
            "  CMake Warning (dev) at CMakeLists.txt:28 (install):\n",
            "    Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
            "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\n",
            "  -- Configuring done (3.9s)\n",
            "  -- Generating done (0.0s)\n",
            "  -- Build files have been written to: /tmp/tmphc8as334/build\n",
            "  *** Building project with Ninja...\n",
            "  Change Dir: '/tmp/tmphc8as334/build'\n",
            "\n",
            "  Run Build Command(s): /tmp/pip-build-env-juvkujf5/normal/local/lib/python3.10/dist-packages/ninja/data/bin/ninja -v\n",
            "  [1/11] /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_USE_CUBLAS -DGGML_USE_K_QUANTS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-lne8uo2c/llama-cpp-python_ff3726da5fba46faab72c74401e060a6/vendor/llama.cpp/. -isystem /usr/local/cuda/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Wno-unused-function -mf16c -mfma -mavx -mavx2 -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o -c /tmp/pip-install-lne8uo2c/llama-cpp-python_ff3726da5fba46faab72c74401e060a6/vendor/llama.cpp/ggml-alloc.c\n",
            "  [2/11] /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_USE_CUBLAS -DGGML_USE_K_QUANTS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-lne8uo2c/llama-cpp-python_ff3726da5fba46faab72c74401e060a6/vendor/llama.cpp/. -isystem /usr/local/cuda/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Wno-unused-function -mf16c -mfma -mavx -mavx2 -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/k_quants.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/k_quants.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/k_quants.c.o -c /tmp/pip-install-lne8uo2c/llama-cpp-python_ff3726da5fba46faab72c74401e060a6/vendor/llama.cpp/k_quants.c\n",
            "  [3/11] /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_USE_CUBLAS -DGGML_USE_K_QUANTS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-lne8uo2c/llama-cpp-python_ff3726da5fba46faab72c74401e060a6/vendor/llama.cpp/. -isystem /usr/local/cuda/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Wno-unused-function -mf16c -mfma -mavx -mavx2 -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o -c /tmp/pip-install-lne8uo2c/llama-cpp-python_ff3726da5fba46faab72c74401e060a6/vendor/llama.cpp/ggml.c\n",
            "  [4/11] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_USE_CUBLAS -DGGML_USE_K_QUANTS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-lne8uo2c/llama-cpp-python_ff3726da5fba46faab72c74401e060a6/vendor/llama.cpp/common/. -I/tmp/pip-install-lne8uo2c/llama-cpp-python_ff3726da5fba46faab72c74401e060a6/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -Wno-format-truncation -Wno-array-bounds -mf16c -mfma -mavx -mavx2 -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -c /tmp/pip-install-lne8uo2c/llama-cpp-python_ff3726da5fba46faab72c74401e060a6/vendor/llama.cpp/common/common.cpp\n",
            "  [5/11] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_USE_CUBLAS -DGGML_USE_K_QUANTS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-lne8uo2c/llama-cpp-python_ff3726da5fba46faab72c74401e060a6/vendor/llama.cpp/common/. -I/tmp/pip-install-lne8uo2c/llama-cpp-python_ff3726da5fba46faab72c74401e060a6/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -Wno-format-truncation -Wno-array-bounds -mf16c -mfma -mavx -mavx2 -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -c /tmp/pip-install-lne8uo2c/llama-cpp-python_ff3726da5fba46faab72c74401e060a6/vendor/llama.cpp/common/console.cpp\n",
            "  [6/11] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_USE_CUBLAS -DGGML_USE_K_QUANTS -DK_QUANTS_PER_ITERATION=2 -DLLAMA_BUILD -DLLAMA_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dllama_EXPORTS -I/tmp/pip-install-lne8uo2c/llama-cpp-python_ff3726da5fba46faab72c74401e060a6/vendor/llama.cpp/. -isystem /usr/local/cuda/include -O3 -DNDEBUG -std=gnu++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -Wno-format-truncation -Wno-array-bounds -mf16c -mfma -mavx -mavx2 -MD -MT vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -MF vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o.d -o vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -c /tmp/pip-install-lne8uo2c/llama-cpp-python_ff3726da5fba46faab72c74401e060a6/vendor/llama.cpp/llama.cpp\n",
            "  [7/11] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_USE_CUBLAS -DGGML_USE_K_QUANTS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-lne8uo2c/llama-cpp-python_ff3726da5fba46faab72c74401e060a6/vendor/llama.cpp/common/. -I/tmp/pip-install-lne8uo2c/llama-cpp-python_ff3726da5fba46faab72c74401e060a6/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -Wno-format-truncation -Wno-array-bounds -mf16c -mfma -mavx -mavx2 -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o -c /tmp/pip-install-lne8uo2c/llama-cpp-python_ff3726da5fba46faab72c74401e060a6/vendor/llama.cpp/common/grammar-parser.cpp\n",
            "  [8/11] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_USE_CUBLAS -DGGML_USE_K_QUANTS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-lne8uo2c/llama-cpp-python_ff3726da5fba46faab72c74401e060a6/vendor/llama.cpp/. -isystem /usr/local/cuda/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -mf16c -mfma -mavx -mavx2 -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o.d -x cu -c /tmp/pip-install-lne8uo2c/llama-cpp-python_ff3726da5fba46faab72c74401e060a6/vendor/llama.cpp/ggml-cuda.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o\n",
            "  [9/11] : && /usr/bin/c++ -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libllama.so -o vendor/llama.cpp/libllama.so vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/k_quants.c.o vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -L/usr/local/cuda/targets/x86_64-linux/lib -Wl,-rpath,/usr/local/cuda-11.8/targets/x86_64-linux/lib:  /usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudart.so  /usr/local/cuda-11.8/targets/x86_64-linux/lib/libcublas.so  /usr/local/cuda-11.8/targets/x86_64-linux/lib/libcublasLt.so  /usr/local/cuda-11.8/targets/x86_64-linux/lib/libculibos.a  -lcudadevrt  -lcudart_static  -lrt  -lpthread  -ldl && :\n",
            "  [10/11] : && /usr/bin/g++ -fPIC  -shared -Wl,-soname,libggml_shared.so -o vendor/llama.cpp/libggml_shared.so vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/k_quants.c.o  /usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudart.so  /usr/local/cuda-11.8/targets/x86_64-linux/lib/libcublas.so  /usr/local/cuda-11.8/targets/x86_64-linux/lib/libcublasLt.so  /usr/local/cuda-11.8/targets/x86_64-linux/lib/libculibos.a  -lcudadevrt  -lcudart_static  -lrt  -lpthread  -ldl -L\"/usr/local/cuda/targets/x86_64-linux/lib/stubs\" -L\"/usr/local/cuda/targets/x86_64-linux/lib\" && :\n",
            "  [11/11] : && /tmp/pip-build-env-juvkujf5/normal/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E rm -f vendor/llama.cpp/libggml_static.a && /usr/bin/ar qc vendor/llama.cpp/libggml_static.a  vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/k_quants.c.o && /usr/bin/ranlib vendor/llama.cpp/libggml_static.a && :\n",
            "\n",
            "  *** Installing project into wheel...\n",
            "  -- Install configuration: \"Release\"\n",
            "  -- Installing: /tmp/tmphc8as334/wheel/platlib/lib/libggml_shared.so\n",
            "  -- Installing: /tmp/tmphc8as334/wheel/platlib/lib/cmake/Llama/LlamaConfig.cmake\n",
            "  -- Installing: /tmp/tmphc8as334/wheel/platlib/lib/cmake/Llama/LlamaConfigVersion.cmake\n",
            "  -- Installing: /tmp/tmphc8as334/wheel/platlib/include/ggml.h\n",
            "  -- Installing: /tmp/tmphc8as334/wheel/platlib/include/ggml-cuda.h\n",
            "  -- Installing: /tmp/tmphc8as334/wheel/platlib/include/k_quants.h\n",
            "  -- Installing: /tmp/tmphc8as334/wheel/platlib/lib/libllama.so\n",
            "  -- Set runtime path of \"/tmp/tmphc8as334/wheel/platlib/lib/libllama.so\" to \"\"\n",
            "  -- Installing: /tmp/tmphc8as334/wheel/platlib/include/llama.h\n",
            "  -- Installing: /tmp/tmphc8as334/wheel/platlib/bin/convert.py\n",
            "  -- Installing: /tmp/tmphc8as334/wheel/platlib/bin/convert-lora-to-ggml.py\n",
            "  -- Installing: /tmp/tmphc8as334/wheel/platlib/llama_cpp/libllama.so\n",
            "  -- Set runtime path of \"/tmp/tmphc8as334/wheel/platlib/llama_cpp/libllama.so\" to \"\"\n",
            "  -- Installing: /tmp/pip-install-lne8uo2c/llama-cpp-python_ff3726da5fba46faab72c74401e060a6/llama_cpp/libllama.so\n",
            "  -- Set runtime path of \"/tmp/pip-install-lne8uo2c/llama-cpp-python_ff3726da5fba46faab72c74401e060a6/llama_cpp/libllama.so\" to \"\"\n",
            "  *** Making wheel...\n",
            "  *** Created llama_cpp_python-0.2.6-cp310-cp310-manylinux_2_35_x86_64.whl...\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.6-cp310-cp310-manylinux_2_35_x86_64.whl size=6197407 sha256=f7924717be71b3bc4800536ef0b810bb47315857317e4630c4c8ae3d7bd01278\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0dt4cihp/wheels/6c/ae/75/c2ad88ef0d1e219f981c51367b8533025345d1a14aa2f09662\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: typing-extensions, numpy, diskcache, llama-cpp-python\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/__pycache__/typing_extensions.cpython-310.pyc\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions-4.5.0.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions.py\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Removing file or directory /usr/local/bin/f2py\n",
            "      Removing file or directory /usr/local/bin/f2py3\n",
            "      Removing file or directory /usr/local/bin/f2py3.10\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy-1.23.5.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy.libs/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy/\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  changing mode of /usr/local/bin/f2py to 755\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.26.0 which is incompatible.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.26.0 which is incompatible.\n",
            "tensorflow 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 1.26.0 which is incompatible.\n",
            "tensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.2.6 numpy-1.26.0 typing-extensions-4.7.1\n",
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.17.1-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.8/294.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.7.22)\n",
            "Installing collected packages: huggingface_hub\n",
            "Successfully installed huggingface_hub-0.17.1\n"
          ]
        }
      ],
      "source": [
        "# GPU llama-cpp-python\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir --verbose\n",
        "# For download the models\n",
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGUF\"\n",
        "model_basename = \"llama-2-13b-chat.Q6_K.gguf\" # the model is in bin format"
      ],
      "metadata": {
        "id": "-PWavpQIkE3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download"
      ],
      "metadata": {
        "id": "FJf68vDekE0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama"
      ],
      "metadata": {
        "id": "DLUIFEx3kEye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "5b5e2d5773e642149a76237b432e39d1",
            "d8fb43b7a7f34365a178ea383d40563a",
            "8111d8b30b6e42bf92bb2c196d8e34f6",
            "377d026d39ac453aac5a3ea8c213ba38",
            "983274f1ff034d76abff6fe9cc5d72be",
            "9884f283e60e4c6f8d747d4173170f2f",
            "a5798dba1abd44b4818ecbe7493ce7f3",
            "8f7196faf17f45f491e309aaa067456a",
            "f3cfa4d39cb841f5b554f0ea93286e4b",
            "a231692ff8734ef1a28ae1af74f158b6",
            "599b4ea3e1114ee19bdbe8f632b04398"
          ]
        },
        "id": "DqopCYtckEu7",
        "outputId": "6f287369-ad6d-43ea-9335-ddca326809c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)2-13b-chat.Q6_K.gguf:   0%|          | 0.00/10.7G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b5e2d5773e642149a76237b432e39d1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU\n",
        "lcpp_llm = None\n",
        "lcpp_llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_threads=2, # CPU cores\n",
        "    n_batch=512, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "    n_gpu_layers=32 # Change this value based on your model and your GPU VRAM pool.\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2Xgb1UtkEsf",
        "outputId": "a4c711d7-2c68-450f-c201-534853ce4371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# See the number of layers in GPU\n",
        "lcpp_llm.params.n_gpu_layers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeihGm3bkEqK",
        "outputId": "7d801f8f-c1a9-4158-aedb-c72a3665766a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classify Prompt and Obtaining Category"
      ],
      "metadata": {
        "id": "dSplACY4uou-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "category_dict = {\n",
        "    \"TraditionalChineseMedicine(TCM)\" : \"TCM means traditional chinese medicine. Please return the names of the hospital in bullet points with the TCM service they offer also in bullet points.\",\n",
        "    \"COVID\" : \"Please do not add too much details. Explain it in a simple and concise way that is straight to the point and cite sources\"\n",
        "}\n",
        "\n",
        "categories_list = ['TraditionalChineseMedicine(TCM)', 'COVID']\n",
        "\n",
        "prompt_classify = \"what is long covid?\"\n",
        "prompt_template_classify =f'''SYSTEM: You are a helpful, respectful and honest assistant. You will be given a prompt that will ask you to classify the user query from CATEGORY_LIST.\n",
        "\n",
        "CATEGORY_LIST : {categories_list}\n",
        "\n",
        "PROMPT: {prompt_classify}. Please only give me the CATEGORY_NAME from {categories_list} that best matches {prompt_classify}\n",
        "\n",
        "ASSISTANT :\n",
        "\n",
        "PROMPT : Please only tell me the category from CATEGORY_LIST with no other text\n",
        "\n",
        "ASSISTANT :\n",
        "'''\n",
        "\n",
        "response_classify=lcpp_llm(prompt=prompt_template_classify, max_tokens=512, temperature=0.0, top_p=1,\n",
        "                  repeat_penalty=1.0, top_k=150,\n",
        "                  echo=True)"
      ],
      "metadata": {
        "id": "S3IIFsF0urOe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "f1a8a982-f58d-43fb-fc79-9d375e6bc30c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-bcb33ec1ae33>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m '''\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m response_classify=lcpp_llm(prompt=prompt_template_classify, max_tokens=512, temperature=0.0, top_p=1,\n\u001b[0m\u001b[1;32m     23\u001b[0m                   \u001b[0mrepeat_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                   echo=True)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'lcpp_llm' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "categories_list = ['TraditionalChineseMedicine(TCM)', 'COVID']\n",
        "user_query = \"which hospitals have tcm\"\n",
        "\n",
        "prompt = \"Categories : \" + str(categories_list) +\"\\n\\nUSER QUERY : \" + user_query + \"\\n\\nINSTRUCTION : Please only give me the category name from the python list I provided in one word\"\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful, respectful and honest assistant. You will be given a prompt that will ask you to classify the user query from a list of categories given.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}  ],\n",
        "  temperature = 0.0\n",
        "  )\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPAPMMwQ9HAP",
        "outputId": "745c72a1-4c8e-4aee-e721-4c4270050b11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TraditionalChineseMedicine(TCM)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Io5skrE0-J30",
        "outputId": "85053368-4319-455f-cf90-ff1d08589c62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categories : ['TraditionalChineseMedicine(TCM)', 'COVID']\n",
            "\n",
            "USER QUERY : which hospitals have tcm\n",
            "\n",
            "INSTRUCTION : Please only give me the category name from the python list I provided in one word\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "category_output = response_classify[\"choices\"][0][\"text\"]\n",
        "print(category_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtbIu4yIX9yr",
        "outputId": "558e3dd8-f778-4e03-8786-3ce0a4f4abc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SYSTEM: You are a helpful, respectful and honest assistant. You will be given a prompt that will ask you to classify the user query from CATEGORY_LIST.\n",
            "\n",
            "CATEGORY_LIST : ['TraditionalChineseMedicine(TCM)', 'COVID']\n",
            "\n",
            "PROMPT: what is long covid?. Please only give me the CATEGORY_NAME from ['TraditionalChineseMedicine(TCM)', 'COVID'] that best matches what is long covid?\n",
            "\n",
            "ASSISTANT : \n",
            "\n",
            "PROMPT : Please only tell me the category from CATEGORY_LIST with no other text\n",
            "\n",
            "ASSISTANT : \n",
            "\n",
            "The category that best matches \"long covid\" is \"COVID\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_category = category_output.replace(\"\\n\",\"\").split(\"'\")\n",
        "prompt_category = prompt_category[-2].replace(\" \",\"\")\n",
        "print(prompt_category)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkUjp8thlgob",
        "outputId": "42bd9cc1-5ea0-47b2-aeaf-56d142c6674e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COVID\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# User prompt"
      ],
      "metadata": {
        "id": "XqlUi8IuuyL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2FxcIS2pWNZ",
        "outputId": "f7b7386e-4a9a-429e-bb93-567902c8f7c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining categories (temporary)"
      ],
      "metadata": {
        "id": "2g7GI7SXrEyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "category_dict = {\n",
        "    \"TraditionalChineseMedicine(TCM)\" : \"TCM means traditional chinese medicine. TCM practitioners use various psychological and/or physical approaches (such as acupuncture and tai chi) as well as herbal products to address health problems\",\n",
        "    \"COVID\" : \"COVID-19 is an infectious disease caused by the SARS-CoV-2 virus. Transmission of the virus occurs primarily through the respiratory droplets of infected people. Your output should be simple and concise way that is straight to the point and cite reputable sources if possible\"\n",
        "}\n",
        "\n",
        "categories_list = ['TraditionalChineseMedicine(TCM)', 'COVID']\n",
        "\n",
        "prompt_category = 'TraditionalChineseMedicine(TCM)'"
      ],
      "metadata": {
        "id": "eNCe5-20ib4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding best prompt (AI generated)\n",
        "\n",
        "1) Obtain additional information from category_dict and append to the user query\n",
        "\n",
        "2) LLM is given system instruction to generate the best prompt according to elderly criteria\n",
        "\n",
        "*   Concise with no additional information\n",
        "*   In short bullet points if possible\n",
        "\n",
        "3) Compare between generated best prompt VS manual crafted prompt\n",
        "\n"
      ],
      "metadata": {
        "id": "Di4p-ChnrH9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "if prompt_category in category_dict :\n",
        "  additional_text = category_dict[prompt_category]\n",
        "else :\n",
        "  additional_text = ''\n",
        "\n",
        "user_prompt = \"which hospitals has tcm?\" + additional_text\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful, respectful and honest assistant. Your job is to generate the best detailed prompt that can give me a short, concise and easy to understand answer. Please include these criteria in the prompt. Please also specify in the prompt that we would like neat, short bullet points if need be. Also this is in the context of Singapore\"},\n",
        "    {\"role\": \"user\", \"content\": user_prompt} ],\n",
        "  temperature = 0.0\n",
        "  )\n",
        "\n",
        "print(response.choices[0].message.content)\n",
        "ai_gen_prompt = response.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qIHMYSPzVf7",
        "outputId": "d7be1243-b547-4c99-8f04-3e2c95e97376"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Which hospitals in Singapore offer Traditional Chinese Medicine (TCM) services? Please provide a list of hospitals that have TCM practitioners and include any additional information about the TCM services they offer. Please present the information in neat, short bullet points if necessary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "if prompt_category in category_dict :\n",
        "  additional_text = category_dict[prompt_category]\n",
        "else :\n",
        "  additional_text = ''\n",
        "\n",
        "user_prompt = \"which hospitals in singapore has tcm?\" + additional_text\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful, respectful and honest assistant. Your job is to generate the best detailed prompt that can give me a short, concise and easy to understand answer for the elderly. Please include these criteria in the prompt. Please also specify in the prompt that we would like neat, short bullet points.\"},\n",
        "    {\"role\": \"user\", \"content\": user_prompt} ],\n",
        "  temperature = 0.1\n",
        "  )\n",
        "\n",
        "print(response.choices[0].message.content)\n",
        "ai_gen_prompt = response.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwfwHtANpWLP",
        "outputId": "b8a52a00-c2cb-4669-ff9f-1cced1f55c64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Which hospitals in Singapore offer Traditional Chinese Medicine (TCM) services?\n",
            "\n",
            "- Please provide a list of hospitals in Singapore that offer TCM services.\n",
            "- Include the names of hospitals that have TCM practitioners.\n",
            "- Specify if the hospitals offer acupuncture, tai chi, herbal products, or other TCM treatments.\n",
            "- Please present the information in short, concise bullet points.\n",
            "- Ensure that the prompt is easy to understand and respectful for the elderly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manually crafted prompt result"
      ],
      "metadata": {
        "id": "yX2nwq6psaar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful, respectful and honest assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"which hospitals has tcm?\"}  ],\n",
        "  temperature = 0.7\n",
        "  )\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7toarg2ZtJN",
        "outputId": "e434006b-e77e-4504-9520-7630f8654003"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traditional Chinese Medicine (TCM) is practiced in various hospitals around the world. Here are some well-known hospitals that offer TCM services:\n",
            "\n",
            "1. Beijing Hospital of Traditional Chinese Medicine - Located in Beijing, China, this hospital specializes in TCM treatments and research.\n",
            "\n",
            "2. Shanghai Traditional Chinese Medicine Hospital - Situated in Shanghai, China, this hospital offers a wide range of TCM services and is known for its integration of modern medicine with TCM.\n",
            "\n",
            "3. Guangzhou University of Chinese Medicine - Located in Guangzhou, China, this university hospital is renowned for its TCM education and research, providing both TCM treatments and modern medical services.\n",
            "\n",
            "4. Singapore General Hospital - This hospital, located in Singapore, has a Traditional Chinese Medicine department that offers TCM consultations, acupuncture, herbal medicine, and other TCM treatments.\n",
            "\n",
            "5. Hong Kong Baptist Hospital - Situated in Hong Kong, this hospital has a TCM department that offers a range of TCM therapies alongside conventional medical treatments.\n",
            "\n",
            "6. Kanazawa University Hospital - Located in Kanazawa, Japan, this hospital has a department of Kampo Medicine, which is the Japanese adaptation of traditional Chinese herbal medicine.\n",
            "\n",
            "Please note that this list is not exhaustive, and there are many other hospitals worldwide that offer TCM services. It's always recommended to research and inquire locally to find the most suitable TCM options available in your area.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AI generated prompt"
      ],
      "metadata": {
        "id": "WAbOhqfBsear"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful, respectful and honest assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": ai_gen_prompt}  ],\n",
        "  temperature = 0.7\n",
        "  )\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xZC7cVAqkrs",
        "outputId": "a1833d6e-5ec0-4b40-c9a5-2b4dfbcb6a2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Certainly! Here is a list of hospitals in Singapore that offer Traditional Chinese Medicine (TCM) services:\n",
            "\n",
            "- Singapore General Hospital (SGH)\n",
            "  - Offers TCM services with acupuncture, herbal products, and tuina massage.\n",
            "  - Has TCM practitioners available.\n",
            "\n",
            "- National University Hospital (NUH)\n",
            "  - Provides TCM services with acupuncture, herbal medicine, and tuina massage.\n",
            "  - Has TCM practitioners available.\n",
            "\n",
            "- Tan Tock Seng Hospital (TTSH)\n",
            "  - Offers TCM services with acupuncture, herbal medicine, and tuina massage.\n",
            "  - Has TCM practitioners available.\n",
            "\n",
            "- Changi General Hospital (CGH)\n",
            "  - Provides TCM services with acupuncture, herbal medicine, and tuina massage.\n",
            "  - Has TCM practitioners available.\n",
            "\n",
            "- Mount Alvernia Hospital\n",
            "  - Offers TCM services with acupuncture, herbal medicine, and tuina massage.\n",
            "  - Has TCM practitioners available.\n",
            "\n",
            "- Thomson Medical Centre\n",
            "  - Provides TCM services with acupuncture, herbal medicine, and tuina massage.\n",
            "  - Has TCM practitioners available.\n",
            "\n",
            "It's important to note that the availability of specific TCM treatments may vary at each hospital. It is recommended to contact the hospitals directly for more detailed information or to schedule an appointment with a TCM practitioner.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AI generated prompt V2"
      ],
      "metadata": {
        "id": "qOnwEIbb0KIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful, respectful and honest assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": ai_gen_prompt}  ],\n",
        "  temperature = 0.7\n",
        "  )\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POzooVjC0MZC",
        "outputId": "779ceb85-eb0d-4237-f197-2208d83996f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is a list of hospitals in Singapore that offer Traditional Chinese Medicine (TCM) services:\n",
            "\n",
            "1. Singapore General Hospital (SGH)\n",
            "   - TCM services available at the SGH TCM Clinic.\n",
            "   - Offers a range of TCM treatments including acupuncture, herbal medicine, and Tui Na massage.\n",
            "\n",
            "2. National University Hospital (NUH)\n",
            "   - NUH Integrative Medicine Clinic provides TCM services.\n",
            "   - TCM treatments offered include acupuncture, herbal medicine, cupping, and gua sha.\n",
            "\n",
            "3. Tan Tock Seng Hospital (TTSH)\n",
            "   - TTSH Chinese Medicine Clinic offers TCM services.\n",
            "   - Provides acupuncture, herbal medicine, and Tui Na massage.\n",
            "\n",
            "4. Changi General Hospital (CGH)\n",
            "   - CGH Chinese Medicine Clinic offers TCM services.\n",
            "   - TCM treatments available include acupuncture, herbal medicine, cupping, and Tui Na massage.\n",
            "\n",
            "5. Mount Alvernia Hospital\n",
            "   - Mount Alvernia TCM Clinic provides TCM services.\n",
            "   - Offers acupuncture, herbal medicine, and Tui Na massage.\n",
            "\n",
            "6. Raffles Hospital\n",
            "   - Raffles Chinese Medicine provides TCM services.\n",
            "   - TCM treatments offered include acupuncture, herbal medicine, and Tui Na massage.\n",
            "\n",
            "Please note that this information is subject to change, and it is recommended to contact the hospitals directly for the most up-to-date information regarding their TCM services.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Putting it all together"
      ],
      "metadata": {
        "id": "ucVxTbKT37uy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining functions"
      ],
      "metadata": {
        "id": "hIcifjn15CH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "category_dict = {\n",
        "    \"TraditionalChineseMedicine(TCM)\" : \"TCM means traditional chinese medicine. TCM practitioners use various psychological and/or physical approaches (such as acupuncture and tai chi) as well as herbal products to address health problems\",\n",
        "    \"COVID\" : \"COVID-19 is an infectious disease caused by the SARS-CoV-2 virus. Transmission of the virus occurs primarily through the respiratory droplets of infected people. Your output should be simple and concise way that is straight to the point and cite reputable sources if possible\"\n",
        "}\n",
        "\n",
        "categories_list = ['TraditionalChineseMedicine(TCM)', 'COVID']\n",
        "\n",
        "def get_category(user_prompt) :\n",
        "  pass\n",
        "\n",
        "def generate_prompt(user_prompt) :\n",
        "  pass\n",
        "\n",
        "def get_prompt(user_prompt) :\n",
        "  pass"
      ],
      "metadata": {
        "id": "cFI1ohkL5AHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt = {}\n",
        "\n",
        "# adding information relevant to the category if category is detected\n",
        "if prompt_category in category_dict :\n",
        "  additional_text = category_dict[prompt_category]\n",
        "else :\n",
        "  additional_text = ''\n",
        "\n",
        "user_prompt = {user_prompt} + additional_text\n",
        "\n"
      ],
      "metadata": {
        "id": "KY5V-_HO392Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZFhu0TyO39yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sJt4LACt39wO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_jqMxOQY39RC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demo"
      ],
      "metadata": {
        "id": "VU_MyK8qTrdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "tHSg-RhBTtUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\n",
        "\n",
        "# model_name = \"../model_weights/13B\"\n",
        "\n",
        "# print(f\"Starting to load the model {model_name} into memory\")\n",
        "\n",
        "# m = AutoModelForCausalLM.from_pretrained(\n",
        "#     model_path,\n",
        "#     #load_in_8bit=True,\n",
        "#     torch_dtype=torch.bfloat16\n",
        "#     # device_map={\"\": 1}\n",
        "# )\n",
        "\n",
        "# tok = LlamaTokenizer.from_pretrained(model_path)\n",
        "# tok.bos_token_id = 1\n",
        "\n",
        "# stop_token_ids = [0]\n",
        "\n",
        "# print(f\"Successfully loaded the model {model_name} into memory\")"
      ],
      "metadata": {
        "id": "EnSLQ-kSTt4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " m = AutoModelForCausalLM.from_pretrained(\n",
        "    model_basename\n",
        "    #load_in_8bit=True,\n",
        "    # torch_dtype=torch.bfloat16\n",
        "    # device_map={\"\": 1}\n",
        ")"
      ],
      "metadata": {
        "id": "UZrSh0p9ICDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import os\n",
        "from threading import Event, Thread\n",
        "from uuid import uuid4\n",
        "\n",
        "import gradio as gr\n",
        "import requests\n",
        "\n",
        "max_new_tokens = 1536\n",
        "start_message = \"\"\"A chat between a friendly chatbot and a human who only asks ONE question.\n",
        "The chatbot will not use the conversation history to answer the next user query no matter what.\n",
        "\"\"\"\n",
        "\n",
        "# start_message = \"\"\"\n",
        "# \"\"\"\n",
        "\n",
        "class StopOnTokens(StoppingCriteria):\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        for stop_id in stop_token_ids:\n",
        "            if input_ids[0][-1] == stop_id:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "\n",
        "def convert_history_to_text(history):\n",
        "    text = start_message + \"\".join(\n",
        "        [\n",
        "            \"\".join(\n",
        "                [\n",
        "                    f\"### Human: {item[0]}\\n\",\n",
        "                    f\"### Assistant: {item[1]}\\n\",\n",
        "                ]\n",
        "            )\n",
        "            for item in history[:-1]\n",
        "        ]\n",
        "    )\n",
        "    text += \"\".join(\n",
        "        [\n",
        "            \"\".join(\n",
        "                [\n",
        "                    f\"### Human: {history[-1][0]}\\n\",\n",
        "                    f\"### Assistant: {history[-1][1]}\\n\",\n",
        "                ]\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "    return text\n",
        "\n",
        "\n",
        "def log_conversation(conversation_id, history, messages, generate_kwargs):\n",
        "    logging_url = os.getenv(\"LOGGING_URL\", None)\n",
        "    if logging_url is None:\n",
        "        return\n",
        "\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
        "\n",
        "    data = {\n",
        "        \"conversation_id\": conversation_id,\n",
        "        \"timestamp\": timestamp,\n",
        "        \"history\": history,\n",
        "        \"messages\": messages,\n",
        "        \"generate_kwargs\": generate_kwargs,\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        requests.post(logging_url, json=data)\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error logging conversation: {e}\")\n",
        "\n",
        "\n",
        "def user(message, history):\n",
        "    # Append the user's message to the conversation history\n",
        "    return \"\", history + [[message, \"\"]]\n",
        "\n",
        "\n",
        "def bot(history, temperature, top_p, top_k, repetition_penalty, conversation_id):\n",
        "    print(f\"history: {history}\")\n",
        "    # Initialize a StopOnTokens object\n",
        "    stop = StopOnTokens()\n",
        "\n",
        "    # Construct the input message string for the model by concatenating the current system message and conversation history\n",
        "    messages = convert_history_to_text(history)\n",
        "\n",
        "    # Tokenize the messages string\n",
        "    input_ids = tok(messages, return_tensors=\"pt\").input_ids\n",
        "    input_ids = input_ids.to(m.device)\n",
        "    streamer = TextIteratorStreamer(tok, timeout=10.0, skip_prompt=True, skip_special_tokens=True)\n",
        "    generate_kwargs = dict(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=temperature,\n",
        "        do_sample=temperature > 0.0,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        streamer=streamer,\n",
        "        stopping_criteria=StoppingCriteriaList([stop]),\n",
        "    )\n",
        "\n",
        "    stream_complete = Event()\n",
        "\n",
        "    def generate_and_signal_complete():\n",
        "        m.generate(**generate_kwargs)\n",
        "        stream_complete.set()\n",
        "\n",
        "    def log_after_stream_complete():\n",
        "        stream_complete.wait()\n",
        "        log_conversation(\n",
        "            conversation_id,\n",
        "            history,\n",
        "            messages,\n",
        "            {\n",
        "                \"top_k\": top_k,\n",
        "                \"top_p\": top_p,\n",
        "                \"temperature\": temperature,\n",
        "                \"repetition_penalty\": repetition_penalty,\n",
        "            },\n",
        "        )\n",
        "\n",
        "    t1 = Thread(target=generate_and_signal_complete)\n",
        "    t1.start()\n",
        "\n",
        "    t2 = Thread(target=log_after_stream_complete)\n",
        "    t2.start()\n",
        "\n",
        "    # Initialize an empty string to store the generated text\n",
        "    partial_text = \"\"\n",
        "    for new_text in streamer:\n",
        "        partial_text += new_text\n",
        "        history[-1][1] = partial_text\n",
        "        yield history\n",
        "\n",
        "\n",
        "def get_uuid():\n",
        "    return str(uuid4())\n",
        "\n",
        "\n",
        "with gr.Blocks(\n",
        "    theme=gr.themes.Soft(),\n",
        "    css=\".disclaimer {font-variant-caps: all-small-caps;}\",\n",
        ") as demo:\n",
        "    conversation_id = gr.State(get_uuid)\n",
        "    gr.Markdown(\n",
        "        \"\"\"<h1><center>LLama 2 13b chat</center></h1>\n",
        "\"\"\"\n",
        "    )\n",
        "    chatbot = gr.Chatbot().style(height=500)\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            msg = gr.Textbox(\n",
        "                label=\"Chat Message Box\",\n",
        "                placeholder=\"Chat Message Box\",\n",
        "                show_label=False,\n",
        "            ).style(container=False)\n",
        "        with gr.Column():\n",
        "            with gr.Row():\n",
        "                submit = gr.Button(\"Submit\")\n",
        "                stop = gr.Button(\"Stop\")\n",
        "                clear = gr.Button(\"Clear\")\n",
        "    with gr.Row():\n",
        "        with gr.Accordion(\"Advanced Options:\", open=False):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    with gr.Row():\n",
        "                        temperature = gr.Slider(\n",
        "                            label=\"Temperature\",\n",
        "                            value=0.0,\n",
        "                            minimum=0.0,\n",
        "                            maximum=1.0,\n",
        "                            step=0.1,\n",
        "                            interactive=True,\n",
        "                            info=\"Higher values produce more diverse outputs\",\n",
        "                        )\n",
        "                with gr.Column():\n",
        "                    with gr.Row():\n",
        "                        top_p = gr.Slider(\n",
        "                            label=\"Top-p (nucleus sampling)\",\n",
        "                            value=1.0,\n",
        "                            minimum=0.0,\n",
        "                            maximum=1,\n",
        "                            step=0.01,\n",
        "                            interactive=True,\n",
        "                            info=(\n",
        "                                \"Sample from the smallest possible set of tokens whose cumulative probability \"\n",
        "                                \"exceeds top_p. Set to 1 to disable and sample from all tokens.\"\n",
        "                            ),\n",
        "                        )\n",
        "                with gr.Column():\n",
        "                    with gr.Row():\n",
        "                        top_k = gr.Slider(\n",
        "                            label=\"Top-k\",\n",
        "                            value=0,\n",
        "                            minimum=0.0,\n",
        "                            maximum=200,\n",
        "                            step=1,\n",
        "                            interactive=True,\n",
        "                            info=\"Sample from a shortlist of top-k tokens — 0 to disable and sample from all tokens.\",\n",
        "                        )\n",
        "                with gr.Column():\n",
        "                    with gr.Row():\n",
        "                        repetition_penalty = gr.Slider(\n",
        "                            label=\"Repetition Penalty\",\n",
        "                            value=1.1,\n",
        "                            minimum=1.0,\n",
        "                            maximum=2.0,\n",
        "                            step=0.1,\n",
        "                            interactive=True,\n",
        "                            info=\"Penalize repetition — 1.0 to disable.\",\n",
        "                        )\n",
        "    with gr.Row():\n",
        "        gr.Markdown(\n",
        "            \"Disclaimer: The model can produce factually incorrect output, and should not be relied on to produce \"\n",
        "            \"factually accurate information. The model was trained on various public datasets; while great efforts \"\n",
        "            \"have been taken to clean the pretraining data, it is possible that this model could generate lewd, \"\n",
        "            \"biased, or otherwise offensive outputs.\",\n",
        "            elem_classes=[\"disclaimer\"],\n",
        "        )\n",
        "    with gr.Row():\n",
        "        gr.Markdown(\n",
        "            \"[Privacy policy](https://gist.github.com/samhavens/c29c68cdcd420a9aa0202d0839876dac)\",\n",
        "            elem_classes=[\"disclaimer\"],\n",
        "        )\n",
        "\n",
        "    submit_event = msg.submit(\n",
        "        fn=user,\n",
        "        inputs=[msg, chatbot],\n",
        "        outputs=[msg, chatbot],\n",
        "        queue=False,\n",
        "    ).then(\n",
        "        fn=bot,\n",
        "        inputs=[\n",
        "            chatbot,\n",
        "            temperature,\n",
        "            top_p,\n",
        "            top_k,\n",
        "            repetition_penalty,\n",
        "            conversation_id,\n",
        "        ],\n",
        "        outputs=chatbot,\n",
        "        queue=True,\n",
        "    )\n",
        "    submit_click_event = submit.click(\n",
        "        fn=user,\n",
        "        inputs=[msg, chatbot],\n",
        "        outputs=[msg, chatbot],\n",
        "        queue=False,\n",
        "    ).then(\n",
        "        fn=bot,\n",
        "        inputs=[\n",
        "            chatbot,\n",
        "            temperature,\n",
        "            top_p,\n",
        "            top_k,\n",
        "            repetition_penalty,\n",
        "            conversation_id,\n",
        "        ],\n",
        "        outputs=chatbot,\n",
        "        queue=True,\n",
        "    )\n",
        "    stop.click(\n",
        "        fn=None,\n",
        "        inputs=None,\n",
        "        outputs=None,\n",
        "        cancels=[submit_event, submit_click_event],\n",
        "        queue=False,\n",
        "    )\n",
        "    clear.click(lambda: None, None, chatbot, queue=False)\n",
        "\n",
        "demo.queue(max_size=128, concurrency_count=2)"
      ],
      "metadata": {
        "id": "DEA-B12nTt0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch()"
      ],
      "metadata": {
        "id": "ekbUXV8BTtyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! nvidia-smi"
      ],
      "metadata": {
        "id": "fOxgHsXUTtvX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}